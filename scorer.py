"""
Charlie Dil 03/12/2022

POSTagger scorer

1. Problem statement
The problem this code exists to solve is the fact that we need a way to evaluate the results generated by the
tagger.py code. That's what this does! It produces a confusion matrix, overall accuracy, and class accuracies (that was
mostly for me).

2. Usage
This code is written in Python 3.10, so please make sure that you run it with that interpreter.
You can run with:

python scorer.py test.txt test_key.txt

Example output:
              DT      RB      ,       PRP     VBD     NNP     .       CC      IN      NN      VB      CD      NNS     :       JJS     JJ      VBN     TO      ``      ''      PRP$    VBP     WP      MD      RP      VBG     VBZ     POS     NNPS    (       )       WRB     WDT     EX      $       JJR     RBR     UH      PDT     WP$     RBS     #       FW      SYM     LS
DT          4738       0       0       0       0       0       0       0      70      16       0       0       0       0       0       2       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
RB            11    1811       0       0       0      37       0       6     286     157       6       0       2       0       2      62       0       0       0       0       0       0       0       0      47       0       0       0       0       0       0       0       0       6       0       0       0       0       0       0       0       0       0       0       0
,              0       0    4554       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
PRP            0       0       0    1042       0       0       0       0       0       0       0       6       0       0       0       0       0       0       0       0       8       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
VBD            0       0       0       0    2179       2       0       0       0     214      15       0       1       0       0       8     289       0       0       0       0       2       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
NNP            5       0       0       0       0    5600       0       0       2      16       2       8       2       0       0      53       0       0       0       0       0       0       0       0       0       0       1       0     304       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
.              0       0       0       0       0       0    4746       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
... cut off here for conciseness
OVERALL ACCURACY:
0.8834747228718914
---------
CLASS ACCURACY
CLASS: DT
0.9817654372150849
-------------------------------

3. Algorithm
Uses similar approach to tagger.py for parsing. Though, it does not care about the actual words, just the order of tags.
The predicted order of tags and the expected order of tags are both passed into sklearn's confusion matrix method
which outputs a matrix where the rows are the actual tag and the columns are the predicted tag. To calculate the overall
accuracy, it sums the diagonal (correct labels) and puts it over the sum of the whole thing. To calculate class
accuracy it takes the number of correctly labeled instances of that class over the total of its row in the matrix.
All of this info is printed out to the console.

"""




import sys
import re

from sklearn.metrics import confusion_matrix
# Collect the command line args
test_file_path = sys.argv[1]
test_key_path = sys.argv[2]
# Open for reading
test_file = open(test_file_path, "r")
test_key = open(test_key_path, "r")

test_file_text = test_file.read()  # save them into strings
test_key_text = test_key.read()

test_file_text = re.sub(r"\\/","",test_file_text)  # there is a really annoying \/ that was messing with my code
test_key_text = re.sub(r"\\/","",test_key_text)  # So I get rid of it.
test_file.close()
test_key.close()  # close them files!


pred = []  # for predicted sequence
true = []  # for correct sequence
labels = []  # store label classes

test_lines = test_file_text.split("\n")
test_key_lines = test_key_text.split("\n")

for l in range(len(test_lines)):
    clean_x = re.sub(r"(^\[\s)|(\s\])", "", test_lines[l])  # no brackets required!
    clean_y = re.sub(r"(^\[\s)|(\s\])", "", test_key_lines[l])
    x_toks = clean_x.split(" ")
    y_toks = clean_y.split(" ")
    for i in range(len(x_toks)):  # go through toks in both files at the same time!!!!!
        if len(x_toks[i].split("/"))>=2:  # just in case we hit maybe an empty line
            x_tag = x_toks[i].split("/")[1]  # tag after /
            y_tag = y_toks[i].split("/")[1]
            if "|" in x_tag:  # just following instructions, taking the 1st tag
                x_tag = x_tag.split("|")[0]
            if "|" in y_tag:
                y_tag = y_tag.split("|")[0]
            if x_tag not in labels:  # make sure it's in labels
                labels.append(x_tag)
            if y_tag not in labels:
                labels.append(y_tag)
        pred.append(x_tag)
        true.append(y_tag)
cf_mat = confusion_matrix(true, pred, labels=labels)  # thanks sklearn!

# Printing results
sum_correct = 0  # used for overall accuracy
for i in range(len(labels)):
    sum_correct+=cf_mat[i][i]  # we add the diagonal
line = "              "  # to make the heading match up right
for l in labels:
    line+="{:8}".format(l)  # header line of matrix, has labels
print(line)
i=0  # used for printing the heading columns
for row in cf_mat:
    line = ""
    first = True  # if its first, print the label
    for val in row:
        if first:  # right here
            line+='{:8}'.format(labels[i])
            first = False  # now we do the rest of the line (the numbers)
            i+=1
        line+='{:8}'.format(val)
    print(line)  # we're printing it one row at a time

print("OVERALL ACCURACY:")
print(sum_correct/len(pred))  # print overall accuracy, calculated here

print("---------\nCLASS ACCURACY")
for i in range(len(labels)):
    print("CLASS: "+labels[i])
    sum_inst = 0  # sum across the row
    for j in range(len(cf_mat[i])):
        sum_inst += cf_mat[i][j]  # done here
    if sum_inst!=0:
        print(cf_mat[i][i]/sum_inst)  # print result of calc
    else:
        print("No instances")  # this doesn't happen
    print("-------------------------------\n")  # for clarity
